---
title: hexo插件开发笔记--hexo-generator-wordcloud
tags: hexo
categories: 开发
wordcloud: true

---

## 前言

我做大作业或毕业设计有个习惯,一定要做有用的东西.所以我的前端大作业打算给我的博客添加一些功能.

项目展示地址: https://m09ic.top , 采用hexo搭建.hexo是一个纯静态博客框架,没有后台管理界面.我采用git与github page完成CI(持续集成)的工作来代替繁琐的服务器管理与后台管理.

博客的搭建过程在此:https://m09ic.top/posts/64867 本文不再叙述.

相关源码会在github开源.

需求是,如果没有指定题图,则生成一个词云图,词云图根据文章内容生成.



分为两步实现,

第一步,使用AI分词将文章内容根据频率转换成json格式,这里我采用盘古分词库来进行中文分词.

第二步,生成词云图.这里我采用echarts的wordcloud模块,

第三步,将功能集成至hexo,并在npm上publish.

需要注意的是,结巴分词与生成词云图对性能消耗较大,而这词云图只要文章内容不改变,是固定了,因此可以在hexo的generator阶段渲染对应的词云图并添加到html中.

## 正文

### 中文分词

链接:https://github.com/leizongmin/node-segment

安装: `npm install segment`

原本想采用结巴分词,因为功能更加强大,但是底层额依赖于C++与python,安装困难.所以采用了盘古中文分词库.

盘古分词除了返回分词结果之外,还能返回词性.

词性是通过16进制标志位来确定的,对应的表如下:

https://github.com/leizongmin/node-segment/blob/master/lib/POSTAG.js

我已经过滤了标点符号与常见无意义的连词,在此之上,还需要过滤一些没有意义的成分,只保留有效的关键字.

例如,连词,助词,量词,介词,数词,叹词,语气词,状态词等等都是无用的.

最后,过滤掉对应的词,并将返回的数字根据频率统计转换为能被echarts-wordcloud解析的库.

js写得有点少,不太熟悉js的函数式编程,这部分的代码非常丑陋.

![image-20200603152549504](D:\Programing\nodejs\hexo-wordcloud\image-20200603152549504.png)

demo code:

### 词云图

链接:https://github.com/ecomfe/echarts-wordcloud

安装:

```
npm install echarts
npm install echarts-wordcloud
```

这里我采用echarts的wordcloud组件.因为使用非常方便.

将上一步生成的数据生成词云图.

![image-20200603145538942](D:\Programing\nodejs\hexo-wordcloud\image-20200603145538942.png)





### hexo-generator-wordcloud

demo已经实现了,接下去就是将demo封装成插件并publish

## Usage

时间匆忙,项目还不完善,暂时未发布到npm与github.

执行 `hexo g`则会自动调用插件给文章生成词云图.

需要在md中的头文件配置来选择是否生成词云图,默认不生成.配置wordcloud为true,则会在hexo generator时生成词云图并保存为`<hexodir>/public/posts/<postid>/wordcloud.png`

头文件配置如下所示:

```
---
title: hexo插件开发笔记--hexo-generator-wordcloud
tags: hexo
categories: 开发
wordcloud: true
---
```

### 效果



## 总结

分词+词云图我之前用python写爬虫的时候实现过,难度并不大.但为一个开源项目添加一个插件还是第一次.

难点主要在对hexo项目结构的理解与二次开发上.